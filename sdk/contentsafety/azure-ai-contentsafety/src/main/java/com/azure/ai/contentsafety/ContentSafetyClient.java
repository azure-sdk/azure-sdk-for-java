// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.
package com.azure.ai.contentsafety;

import com.azure.ai.contentsafety.implementation.ContentSafetyClientImpl;
import com.azure.ai.contentsafety.models.AnalyzeImageOptions;
import com.azure.ai.contentsafety.models.AnalyzeImageResult;
import com.azure.ai.contentsafety.models.AnalyzeTextOptions;
import com.azure.ai.contentsafety.models.AnalyzeTextResult;
import com.azure.ai.contentsafety.models.ContentSafetyImageData;
import com.azure.ai.contentsafety.models.RaiPolicyAnalyzeOption;
import com.azure.ai.contentsafety.models.RaiPolicyAnalyzeResponse;
import com.azure.core.annotation.Generated;
import com.azure.core.annotation.ReturnType;
import com.azure.core.annotation.ServiceClient;
import com.azure.core.annotation.ServiceMethod;
import com.azure.core.exception.ClientAuthenticationException;
import com.azure.core.exception.HttpResponseException;
import com.azure.core.exception.ResourceModifiedException;
import com.azure.core.exception.ResourceNotFoundException;
import com.azure.core.http.rest.RequestOptions;
import com.azure.core.http.rest.Response;
import com.azure.core.util.BinaryData;

/**
 * Initializes a new instance of the synchronous ContentSafetyClient type.
 */
@ServiceClient(builder = ContentSafetyClientBuilder.class)
public final class ContentSafetyClient {

    @Generated
    private final ContentSafetyClientImpl serviceClient;

    /**
     * Initializes an instance of ContentSafetyClient class.
     *
     * @param serviceClient the service client implementation.
     */
    @Generated
    ContentSafetyClient(ContentSafetyClientImpl serviceClient) {
        this.serviceClient = serviceClient;
    }

    /**
     * Analyze Text
     *
     * A synchronous API for the analysis of potentially harmful text content. Currently, it supports four categories:
     * Hate, SelfHarm, Sexual, and Violence.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>
     * {@code
     * {
     *     text: String (Required)
     *     categories (Optional): [
     *         String(Hate/SelfHarm/Sexual/Violence) (Optional)
     *     ]
     *     blocklistNames (Optional): [
     *         String (Optional)
     *     ]
     *     haltOnBlocklistHit: Boolean (Optional)
     *     outputType: String(FourSeverityLevels/EightSeverityLevels) (Optional)
     * }
     * }
     * </pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>
     * {@code
     * {
     *     blocklistsMatch (Optional): [
     *          (Optional){
     *             blocklistName: String (Required)
     *             blocklistItemId: String (Required)
     *             blocklistItemText: String (Required)
     *         }
     *     ]
     *     categoriesAnalysis (Required): [
     *          (Required){
     *             category: String(Hate/SelfHarm/Sexual/Violence) (Required)
     *             severity: Integer (Optional)
     *         }
     *     ]
     * }
     * }
     * </pre>
     *
     * @param options The text analysis request.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the text analysis response along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> analyzeTextWithResponse(BinaryData options, RequestOptions requestOptions) {
        return this.serviceClient.analyzeTextWithResponse(options, requestOptions);
    }

    /**
     * Analyze Image
     *
     * A synchronous API for the analysis of potentially harmful image content. Currently, it supports four categories:
     * Hate, SelfHarm, Sexual, and Violence.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>
     * {@code
     * {
     *     image (Required): {
     *         content: byte[] (Optional)
     *         blobUrl: String (Optional)
     *     }
     *     categories (Optional): [
     *         String(Hate/SelfHarm/Sexual/Violence) (Optional)
     *     ]
     *     outputType: String(FourSeverityLevels) (Optional)
     * }
     * }
     * </pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>
     * {@code
     * {
     *     categoriesAnalysis (Required): [
     *          (Required){
     *             category: String(Hate/SelfHarm/Sexual/Violence) (Required)
     *             severity: Integer (Optional)
     *         }
     *     ]
     * }
     * }
     * </pre>
     *
     * @param options The image analysis request.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return the image analysis response along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> analyzeImageWithResponse(BinaryData options, RequestOptions requestOptions) {
        return this.serviceClient.analyzeImageWithResponse(options, requestOptions);
    }

    /**
     * Analyze Text
     *
     * A synchronous API for the analysis of potentially harmful text content. Currently, it supports four categories:
     * Hate, SelfHarm, Sexual, and Violence.
     *
     * @param options The text analysis request.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the text analysis response.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public AnalyzeTextResult analyzeText(AnalyzeTextOptions options) {
        // Generated convenience method for analyzeTextWithResponse
        RequestOptions requestOptions = new RequestOptions();
        return analyzeTextWithResponse(BinaryData.fromObject(options), requestOptions).getValue()
            .toObject(AnalyzeTextResult.class);
    }

    /**
     * Analyze Text
     *
     * A synchronous API for the analysis of potentially harmful text content. Currently, it supports four categories:
     * Hate, SelfHarm, Sexual, and Violence.
     *
     * @param text The text analysis request.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the text analysis response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public AnalyzeTextResult analyzeText(String text) {
        // Customized convenience method for analyzeTextWithResponse
        AnalyzeTextOptions options = new AnalyzeTextOptions(text);
        return analyzeText(options);
    }

    /**
     * Analyze Image
     *
     * A synchronous API for the analysis of potentially harmful image content. Currently, it supports four categories:
     * Hate, SelfHarm, Sexual, and Violence.
     *
     * @param options The image analysis request.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the image analysis response.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public AnalyzeImageResult analyzeImage(AnalyzeImageOptions options) {
        // Generated convenience method for analyzeImageWithResponse
        RequestOptions requestOptions = new RequestOptions();
        return analyzeImageWithResponse(BinaryData.fromObject(options), requestOptions).getValue()
            .toObject(AnalyzeImageResult.class);
    }

    /**
     * Analyze Image
     *
     * A synchronous API for the analysis of potentially harmful image content. Currently, it supports four categories:
     * Hate, SelfHarm, Sexual, and Violence.
     *
     * @param blobUrl The image analysis request.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the image analysis response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public AnalyzeImageResult analyzeImage(String blobUrl) {
        // Customized convenience method for analyzeImageWithResponse
        AnalyzeImageOptions body = new AnalyzeImageOptions(new ContentSafetyImageData().setBlobUrl(blobUrl));
        return analyzeImage(body);
    }

    /**
     * Analyze Image
     *
     * A synchronous API for the analysis of potentially harmful image content. Currently, it supports four categories:
     * Hate, SelfHarm, Sexual, and Violence.
     *
     * @param content The image analysis request.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the image analysis response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public AnalyzeImageResult analyzeImage(BinaryData content) {
        // Customized convenience method for analyzeImageWithResponse
        AnalyzeImageOptions body = new AnalyzeImageOptions(new ContentSafetyImageData().setContent(content));
        return analyzeImage(body);
    }

    /**
     * Rai Policy Analysis
     *
     * A synchronous API for the rai policy analysis of input content.
     * <p><strong>Request Body Schema</strong></p>
     * 
     * <pre>
     * {@code
     * {
     *     messages (Required): [
     *          (Required){
     *             source: String(messageToAI/messageFromAI) (Required)
     *             role: String(all/user/system/assistant/tool) (Required)
     *             contents (Required): [
     *                  (Required){
     *                     kind: String(text/image) (Required)
     *                     text: String (Optional)
     *                     imageBase64: String (Optional)
     *                     imageBlob: String (Optional)
     *                 }
     *             ]
     *         }
     *     ]
     *     raiPolicyInline (Optional): {
     *         name: String (Required)
     *         taskSettings (Optional, Required on create): [
     *              (Optional, Required on create){
     *                 settingId: String (Optional, Required on create)
     *                 settingEnabled: boolean (Optional, Required on create)
     *                 appliedFor (Optional, Required on create): [
     *                      (Optional, Required on create){
     *                         role: String(all/user/system/assistant/tool) (Optional, Required on create)
     *                         source: String(messageToAI/messageFromAI) (Optional, Required on create)
     *                     }
     *                 ]
     *                 kind: String(harmCategory/blocklist/safetyIncident/customHarmCategory) (Optional, Required on create)
     *                 harmCategoryTaskSetting (Optional): {
     *                     harmCategory: String(celebrity/drug/hate/promptInjection/protectedMaterial/sexual/selfHarm/violence) (Optional, Required on create)
     *                     harmConfigId: String (Optional, Required on create)
     *                 }
     *                 blocklistTaskSetting (Optional): {
     *                     name: String (Optional, Required on create)
     *                 }
     *                 safetyIncidentTaskSetting (Optional): {
     *                     name: String (Optional, Required on create)
     *                 }
     *                 customHarmCategoryTaskSetting (Optional): {
     *                     name: String (Optional, Required on create)
     *                 }
     *                 blockingCriteria (Optional, Required on create): {
     *                     enabled: boolean (Optional, Required on create)
     *                     kind: String(severity/riskLevel/isDetected) (Optional, Required on create)
     *                     allowedSeverity: int (Optional, Required on create)
     *                     isDetected: boolean (Optional, Required on create)
     *                     allowedRiskLevel: String(safe/low/medium/high) (Optional, Required on create)
     *                 }
     *             }
     *         ]
     *     }
     *     raiPolicyName: String (Optional)
     * }
     * }
     * </pre>
     * 
     * <p><strong>Response Body Schema</strong></p>
     * 
     * <pre>
     * {@code
     * {
     *     taskResults (Required): [
     *          (Required){
     *             settingId: String (Required)
     *             resultCode: String(oK/noValidInput/internalTimeout/internalError) (Required)
     *             resultCodeDetail: String (Required)
     *             isBlockingCriteriaMet: boolean (Required)
     *             kind: String(harmCategory/blocklist/safetyIncident/customHarmCategory) (Required)
     *             harmCategoryTaskResult (Optional): {
     *                 harmCategory: String(celebrity/drug/hate/promptInjection/protectedMaterial/sexual/selfHarm/violence) (Required)
     *                 harmConfigId: String (Required)
     *                 isDetected: boolean (Required)
     *                 severity: int (Required)
     *                 riskLevel: String(safe/low/medium/high) (Required)
     *                 details (Optional): {
     *                     promptInjection: Boolean (Optional)
     *                     crossDomain: Boolean (Optional)
     *                 }
     *             }
     *             blocklistTaskResult (Optional): {
     *                 name: String (Required)
     *                 isDetected: boolean (Required)
     *             }
     *             safetyIncidentTaskResult (Optional): {
     *                 name: String (Required)
     *                 isDetected: boolean (Required)
     *             }
     *             customCategoryTaskResult (Optional): {
     *                 name: String (Required)
     *                 isDetected: boolean (Required)
     *             }
     *         }
     *     ]
     * }
     * }
     * </pre>
     *
     * @param options The rai policy analyze request.
     * @param requestOptions The options to configure the HTTP request before HTTP client sends it.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @return represents the analysis response obtained by applying a rai policy along with {@link Response}.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<BinaryData> analyzeWithRaiPolicyWithResponse(BinaryData options, RequestOptions requestOptions) {
        return this.serviceClient.analyzeWithRaiPolicyWithResponse(options, requestOptions);
    }

    /**
     * Rai Policy Analysis
     *
     * A synchronous API for the rai policy analysis of input content.
     *
     * @param options The rai policy analyze request.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws HttpResponseException thrown if the request is rejected by server.
     * @throws ClientAuthenticationException thrown if the request is rejected by server on status code 401.
     * @throws ResourceNotFoundException thrown if the request is rejected by server on status code 404.
     * @throws ResourceModifiedException thrown if the request is rejected by server on status code 409.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return represents the analysis response obtained by applying a rai policy.
     */
    @Generated
    @ServiceMethod(returns = ReturnType.SINGLE)
    public RaiPolicyAnalyzeResponse analyzeWithRaiPolicy(RaiPolicyAnalyzeOption options) {
        // Generated convenience method for analyzeWithRaiPolicyWithResponse
        RequestOptions requestOptions = new RequestOptions();
        return analyzeWithRaiPolicyWithResponse(BinaryData.fromObject(options), requestOptions).getValue()
            .toObject(RaiPolicyAnalyzeResponse.class);
    }
}
