// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.resourcemanager.machinelearning.implementation;

import com.azure.core.annotation.BodyParam;
import com.azure.core.annotation.Delete;
import com.azure.core.annotation.ExpectedResponses;
import com.azure.core.annotation.Get;
import com.azure.core.annotation.HeaderParam;
import com.azure.core.annotation.Headers;
import com.azure.core.annotation.Host;
import com.azure.core.annotation.HostParam;
import com.azure.core.annotation.Patch;
import com.azure.core.annotation.PathParam;
import com.azure.core.annotation.Put;
import com.azure.core.annotation.QueryParam;
import com.azure.core.annotation.ReturnType;
import com.azure.core.annotation.ServiceInterface;
import com.azure.core.annotation.ServiceMethod;
import com.azure.core.annotation.UnexpectedResponseExceptionType;
import com.azure.core.http.rest.PagedFlux;
import com.azure.core.http.rest.PagedIterable;
import com.azure.core.http.rest.PagedResponse;
import com.azure.core.http.rest.PagedResponseBase;
import com.azure.core.http.rest.Response;
import com.azure.core.http.rest.RestProxy;
import com.azure.core.management.exception.ManagementException;
import com.azure.core.management.polling.PollResult;
import com.azure.core.util.Context;
import com.azure.core.util.FluxUtil;
import com.azure.core.util.polling.PollerFlux;
import com.azure.core.util.polling.SyncPoller;
import com.azure.resourcemanager.machinelearning.fluent.InferenceEndpointsClient;
import com.azure.resourcemanager.machinelearning.fluent.models.InferenceEndpointInner;
import com.azure.resourcemanager.machinelearning.models.InferenceEndpointTrackedResourceArmPaginatedResult;
import com.azure.resourcemanager.machinelearning.models.OrderString;
import java.nio.ByteBuffer;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

/**
 * An instance of this class provides access to all the operations defined in InferenceEndpointsClient.
 */
public final class InferenceEndpointsClientImpl implements InferenceEndpointsClient {
    /**
     * The proxy service used to perform REST calls.
     */
    private final InferenceEndpointsService service;

    /**
     * The service client containing this operation class.
     */
    private final AzureMachineLearningServicesImpl client;

    /**
     * Initializes an instance of InferenceEndpointsClientImpl.
     * 
     * @param client the instance of the service client containing this operation class.
     */
    InferenceEndpointsClientImpl(AzureMachineLearningServicesImpl client) {
        this.service = RestProxy.create(InferenceEndpointsService.class, client.getHttpPipeline(),
            client.getSerializerAdapter());
        this.client = client;
    }

    /**
     * The interface defining all the services for AzureMachineLearningServicesInferenceEndpoints to be used by the
     * proxy service to perform REST calls.
     */
    @Host("{$host}")
    @ServiceInterface(name = "AzureMachineLearning")
    public interface InferenceEndpointsService {
        @Headers({ "Content-Type: application/json" })
        @Get("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/inferencePools/{poolName}/endpoints")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<InferenceEndpointTrackedResourceArmPaginatedResult>> list(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("poolName") String poolName, @QueryParam("api-version") String apiVersion,
            @QueryParam("count") Integer count, @QueryParam("$skip") String skip, @QueryParam("tags") String tags,
            @QueryParam("properties") String properties, @QueryParam("orderBy") OrderString orderBy,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Delete("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/inferencePools/{poolName}/endpoints/{endpointName}")
        @ExpectedResponses({ 200, 202, 204 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<Flux<ByteBuffer>>> delete(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("poolName") String poolName, @PathParam("endpointName") String endpointName,
            @QueryParam("api-version") String apiVersion, @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/inferencePools/{poolName}/endpoints/{endpointName}")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<InferenceEndpointInner>> get(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("poolName") String poolName, @PathParam("endpointName") String endpointName,
            @QueryParam("api-version") String apiVersion, @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Patch("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/inferencePools/{poolName}/endpoints/{endpointName}")
        @ExpectedResponses({ 200, 202 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<Flux<ByteBuffer>>> update(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("poolName") String poolName, @PathParam("endpointName") String endpointName,
            @QueryParam("api-version") String apiVersion, @BodyParam("application/json") Object body,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Put("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/inferencePools/{poolName}/endpoints/{endpointName}")
        @ExpectedResponses({ 200, 201 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<Flux<ByteBuffer>>> createOrUpdate(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("poolName") String poolName, @PathParam("endpointName") String endpointName,
            @QueryParam("api-version") String apiVersion, @BodyParam("application/json") InferenceEndpointInner body,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("{nextLink}")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<InferenceEndpointTrackedResourceArmPaginatedResult>> listNext(
            @PathParam(value = "nextLink", encoded = true) String nextLink, @HostParam("$host") String endpoint,
            @HeaderParam("Accept") String accept, Context context);
    }

    /**
     * List Inference Endpoints.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName Name of the InferencePool.
     * @param count Number of InferenceEndpoint to be retrieved in a page of results.
     * @param skip Continuation token for pagination.
     * @param tags A set of tags with which to filter the returned models. It is a comma separated string of tags key or
     * tags key=value. Example: tagKey1,tagKey2,tagKey3=value3 .
     * @param properties A set of properties with which to filter the returned models. It is a comma separated string of
     * properties key and/or properties key=value Example: propKey1,propKey2,propKey3=value3 .
     * @param orderBy The option to order the response.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities along with {@link PagedResponse} on successful completion
     * of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<PagedResponse<InferenceEndpointInner>> listSinglePageAsync(String resourceGroupName,
        String workspaceName, String poolName, Integer count, String skip, String tags, String properties,
        OrderString orderBy) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.list(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, poolName, this.client.getApiVersion(), count, skip, tags, properties,
                orderBy, accept, context))
            .<PagedResponse<InferenceEndpointInner>>map(res -> new PagedResponseBase<>(res.getRequest(),
                res.getStatusCode(), res.getHeaders(), res.getValue().value(), res.getValue().nextLink(), null))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * List Inference Endpoints.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName Name of the InferencePool.
     * @param count Number of InferenceEndpoint to be retrieved in a page of results.
     * @param skip Continuation token for pagination.
     * @param tags A set of tags with which to filter the returned models. It is a comma separated string of tags key or
     * tags key=value. Example: tagKey1,tagKey2,tagKey3=value3 .
     * @param properties A set of properties with which to filter the returned models. It is a comma separated string of
     * properties key and/or properties key=value Example: propKey1,propKey2,propKey3=value3 .
     * @param orderBy The option to order the response.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities along with {@link PagedResponse} on successful completion
     * of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<PagedResponse<InferenceEndpointInner>> listSinglePageAsync(String resourceGroupName,
        String workspaceName, String poolName, Integer count, String skip, String tags, String properties,
        OrderString orderBy, Context context) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        final String accept = "application/json";
        context = this.client.mergeContext(context);
        return service
            .list(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName, workspaceName,
                poolName, this.client.getApiVersion(), count, skip, tags, properties, orderBy, accept, context)
            .map(res -> new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(),
                res.getValue().value(), res.getValue().nextLink(), null));
    }

    /**
     * List Inference Endpoints.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName Name of the InferencePool.
     * @param count Number of InferenceEndpoint to be retrieved in a page of results.
     * @param skip Continuation token for pagination.
     * @param tags A set of tags with which to filter the returned models. It is a comma separated string of tags key or
     * tags key=value. Example: tagKey1,tagKey2,tagKey3=value3 .
     * @param properties A set of properties with which to filter the returned models. It is a comma separated string of
     * properties key and/or properties key=value Example: propKey1,propKey2,propKey3=value3 .
     * @param orderBy The option to order the response.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities as paginated response with {@link PagedFlux}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    private PagedFlux<InferenceEndpointInner> listAsync(String resourceGroupName, String workspaceName, String poolName,
        Integer count, String skip, String tags, String properties, OrderString orderBy) {
        return new PagedFlux<>(() -> listSinglePageAsync(resourceGroupName, workspaceName, poolName, count, skip, tags,
            properties, orderBy), nextLink -> listNextSinglePageAsync(nextLink));
    }

    /**
     * List Inference Endpoints.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName Name of the InferencePool.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities as paginated response with {@link PagedFlux}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    private PagedFlux<InferenceEndpointInner> listAsync(String resourceGroupName, String workspaceName,
        String poolName) {
        final Integer count = null;
        final String skip = null;
        final String tags = null;
        final String properties = null;
        final OrderString orderBy = null;
        return new PagedFlux<>(() -> listSinglePageAsync(resourceGroupName, workspaceName, poolName, count, skip, tags,
            properties, orderBy), nextLink -> listNextSinglePageAsync(nextLink));
    }

    /**
     * List Inference Endpoints.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName Name of the InferencePool.
     * @param count Number of InferenceEndpoint to be retrieved in a page of results.
     * @param skip Continuation token for pagination.
     * @param tags A set of tags with which to filter the returned models. It is a comma separated string of tags key or
     * tags key=value. Example: tagKey1,tagKey2,tagKey3=value3 .
     * @param properties A set of properties with which to filter the returned models. It is a comma separated string of
     * properties key and/or properties key=value Example: propKey1,propKey2,propKey3=value3 .
     * @param orderBy The option to order the response.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities as paginated response with {@link PagedFlux}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    private PagedFlux<InferenceEndpointInner> listAsync(String resourceGroupName, String workspaceName, String poolName,
        Integer count, String skip, String tags, String properties, OrderString orderBy, Context context) {
        return new PagedFlux<>(() -> listSinglePageAsync(resourceGroupName, workspaceName, poolName, count, skip, tags,
            properties, orderBy, context), nextLink -> listNextSinglePageAsync(nextLink, context));
    }

    /**
     * List Inference Endpoints.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName Name of the InferencePool.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities as paginated response with {@link PagedIterable}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    public PagedIterable<InferenceEndpointInner> list(String resourceGroupName, String workspaceName, String poolName) {
        final Integer count = null;
        final String skip = null;
        final String tags = null;
        final String properties = null;
        final OrderString orderBy = null;
        return new PagedIterable<>(
            listAsync(resourceGroupName, workspaceName, poolName, count, skip, tags, properties, orderBy));
    }

    /**
     * List Inference Endpoints.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName Name of the InferencePool.
     * @param count Number of InferenceEndpoint to be retrieved in a page of results.
     * @param skip Continuation token for pagination.
     * @param tags A set of tags with which to filter the returned models. It is a comma separated string of tags key or
     * tags key=value. Example: tagKey1,tagKey2,tagKey3=value3 .
     * @param properties A set of properties with which to filter the returned models. It is a comma separated string of
     * properties key and/or properties key=value Example: propKey1,propKey2,propKey3=value3 .
     * @param orderBy The option to order the response.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities as paginated response with {@link PagedIterable}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    public PagedIterable<InferenceEndpointInner> list(String resourceGroupName, String workspaceName, String poolName,
        Integer count, String skip, String tags, String properties, OrderString orderBy, Context context) {
        return new PagedIterable<>(
            listAsync(resourceGroupName, workspaceName, poolName, count, skip, tags, properties, orderBy, context));
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<Flux<ByteBuffer>>> deleteWithResponseAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        if (endpointName == null) {
            return Mono.error(new IllegalArgumentException("Parameter endpointName is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.delete(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, poolName, endpointName, this.client.getApiVersion(), accept, context))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<Flux<ByteBuffer>>> deleteWithResponseAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, Context context) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        if (endpointName == null) {
            return Mono.error(new IllegalArgumentException("Parameter endpointName is required and cannot be null."));
        }
        final String accept = "application/json";
        context = this.client.mergeContext(context);
        return service.delete(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
            workspaceName, poolName, endpointName, this.client.getApiVersion(), accept, context);
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private PollerFlux<PollResult<Void>, Void> beginDeleteAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName) {
        Mono<Response<Flux<ByteBuffer>>> mono
            = deleteWithResponseAsync(resourceGroupName, workspaceName, poolName, endpointName);
        return this.client.<Void, Void>getLroResult(mono, this.client.getHttpPipeline(), Void.class, Void.class,
            this.client.getContext());
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private PollerFlux<PollResult<Void>, Void> beginDeleteAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, Context context) {
        context = this.client.mergeContext(context);
        Mono<Response<Flux<ByteBuffer>>> mono
            = deleteWithResponseAsync(resourceGroupName, workspaceName, poolName, endpointName, context);
        return this.client.<Void, Void>getLroResult(mono, this.client.getHttpPipeline(), Void.class, Void.class,
            context);
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<Void>, Void> beginDelete(String resourceGroupName, String workspaceName,
        String poolName, String endpointName) {
        return this.beginDeleteAsync(resourceGroupName, workspaceName, poolName, endpointName).getSyncPoller();
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<Void>, Void> beginDelete(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, Context context) {
        return this.beginDeleteAsync(resourceGroupName, workspaceName, poolName, endpointName, context).getSyncPoller();
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return A {@link Mono} that completes when a successful response is received.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Void> deleteAsync(String resourceGroupName, String workspaceName, String poolName,
        String endpointName) {
        return beginDeleteAsync(resourceGroupName, workspaceName, poolName, endpointName).last()
            .flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return A {@link Mono} that completes when a successful response is received.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Void> deleteAsync(String resourceGroupName, String workspaceName, String poolName, String endpointName,
        Context context) {
        return beginDeleteAsync(resourceGroupName, workspaceName, poolName, endpointName, context).last()
            .flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public void delete(String resourceGroupName, String workspaceName, String poolName, String endpointName) {
        deleteAsync(resourceGroupName, workspaceName, poolName, endpointName).block();
    }

    /**
     * Delete InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public void delete(String resourceGroupName, String workspaceName, String poolName, String endpointName,
        Context context) {
        deleteAsync(resourceGroupName, workspaceName, poolName, endpointName, context).block();
    }

    /**
     * Get InferenceEndpoint.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return inferenceEndpoint along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<InferenceEndpointInner>> getWithResponseAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        if (endpointName == null) {
            return Mono.error(new IllegalArgumentException("Parameter endpointName is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.get(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, poolName, endpointName, this.client.getApiVersion(), accept, context))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Get InferenceEndpoint.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return inferenceEndpoint along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<InferenceEndpointInner>> getWithResponseAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, Context context) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        if (endpointName == null) {
            return Mono.error(new IllegalArgumentException("Parameter endpointName is required and cannot be null."));
        }
        final String accept = "application/json";
        context = this.client.mergeContext(context);
        return service.get(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName, workspaceName,
            poolName, endpointName, this.client.getApiVersion(), accept, context);
    }

    /**
     * Get InferenceEndpoint.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return inferenceEndpoint on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<InferenceEndpointInner> getAsync(String resourceGroupName, String workspaceName, String poolName,
        String endpointName) {
        return getWithResponseAsync(resourceGroupName, workspaceName, poolName, endpointName)
            .flatMap(res -> Mono.justOrEmpty(res.getValue()));
    }

    /**
     * Get InferenceEndpoint.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return inferenceEndpoint along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<InferenceEndpointInner> getWithResponse(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, Context context) {
        return getWithResponseAsync(resourceGroupName, workspaceName, poolName, endpointName, context).block();
    }

    /**
     * Get InferenceEndpoint.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return inferenceEndpoint.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public InferenceEndpointInner get(String resourceGroupName, String workspaceName, String poolName,
        String endpointName) {
        return getWithResponse(resourceGroupName, workspaceName, poolName, endpointName, Context.NONE).getValue();
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<Flux<ByteBuffer>>> updateWithResponseAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, Object body) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        if (endpointName == null) {
            return Mono.error(new IllegalArgumentException("Parameter endpointName is required and cannot be null."));
        }
        if (body == null) {
            return Mono.error(new IllegalArgumentException("Parameter body is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(
                context -> service.update(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
                    workspaceName, poolName, endpointName, this.client.getApiVersion(), body, accept, context))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<Flux<ByteBuffer>>> updateWithResponseAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, Object body, Context context) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        if (endpointName == null) {
            return Mono.error(new IllegalArgumentException("Parameter endpointName is required and cannot be null."));
        }
        if (body == null) {
            return Mono.error(new IllegalArgumentException("Parameter body is required and cannot be null."));
        }
        final String accept = "application/json";
        context = this.client.mergeContext(context);
        return service.update(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
            workspaceName, poolName, endpointName, this.client.getApiVersion(), body, accept, context);
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private PollerFlux<PollResult<InferenceEndpointInner>, InferenceEndpointInner> beginUpdateAsync(
        String resourceGroupName, String workspaceName, String poolName, String endpointName, Object body) {
        Mono<Response<Flux<ByteBuffer>>> mono
            = updateWithResponseAsync(resourceGroupName, workspaceName, poolName, endpointName, body);
        return this.client.<InferenceEndpointInner, InferenceEndpointInner>getLroResult(mono,
            this.client.getHttpPipeline(), InferenceEndpointInner.class, InferenceEndpointInner.class,
            this.client.getContext());
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private PollerFlux<PollResult<InferenceEndpointInner>, InferenceEndpointInner> beginUpdateAsync(
        String resourceGroupName, String workspaceName, String poolName, String endpointName, Object body,
        Context context) {
        context = this.client.mergeContext(context);
        Mono<Response<Flux<ByteBuffer>>> mono
            = updateWithResponseAsync(resourceGroupName, workspaceName, poolName, endpointName, body, context);
        return this.client.<InferenceEndpointInner, InferenceEndpointInner>getLroResult(mono,
            this.client.getHttpPipeline(), InferenceEndpointInner.class, InferenceEndpointInner.class, context);
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<InferenceEndpointInner>, InferenceEndpointInner> beginUpdate(String resourceGroupName,
        String workspaceName, String poolName, String endpointName, Object body) {
        return this.beginUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body).getSyncPoller();
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<InferenceEndpointInner>, InferenceEndpointInner> beginUpdate(String resourceGroupName,
        String workspaceName, String poolName, String endpointName, Object body, Context context) {
        return this.beginUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body, context)
            .getSyncPoller();
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<InferenceEndpointInner> updateAsync(String resourceGroupName, String workspaceName, String poolName,
        String endpointName, Object body) {
        return beginUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body).last()
            .flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<InferenceEndpointInner> updateAsync(String resourceGroupName, String workspaceName, String poolName,
        String endpointName, Object body, Context context) {
        return beginUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body, context).last()
            .flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public InferenceEndpointInner update(String resourceGroupName, String workspaceName, String poolName,
        String endpointName, Object body) {
        return updateAsync(resourceGroupName, workspaceName, poolName, endpointName, body).block();
    }

    /**
     * Update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body Online Endpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public InferenceEndpointInner update(String resourceGroupName, String workspaceName, String poolName,
        String endpointName, Object body, Context context) {
        return updateAsync(resourceGroupName, workspaceName, poolName, endpointName, body, context).block();
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<Flux<ByteBuffer>>> createOrUpdateWithResponseAsync(String resourceGroupName,
        String workspaceName, String poolName, String endpointName, InferenceEndpointInner body) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        if (endpointName == null) {
            return Mono.error(new IllegalArgumentException("Parameter endpointName is required and cannot be null."));
        }
        if (body == null) {
            return Mono.error(new IllegalArgumentException("Parameter body is required and cannot be null."));
        } else {
            body.validate();
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.createOrUpdate(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, poolName, endpointName, this.client.getApiVersion(), body, accept,
                context))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<Flux<ByteBuffer>>> createOrUpdateWithResponseAsync(String resourceGroupName,
        String workspaceName, String poolName, String endpointName, InferenceEndpointInner body, Context context) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (poolName == null) {
            return Mono.error(new IllegalArgumentException("Parameter poolName is required and cannot be null."));
        }
        if (endpointName == null) {
            return Mono.error(new IllegalArgumentException("Parameter endpointName is required and cannot be null."));
        }
        if (body == null) {
            return Mono.error(new IllegalArgumentException("Parameter body is required and cannot be null."));
        } else {
            body.validate();
        }
        final String accept = "application/json";
        context = this.client.mergeContext(context);
        return service.createOrUpdate(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
            workspaceName, poolName, endpointName, this.client.getApiVersion(), body, accept, context);
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private PollerFlux<PollResult<InferenceEndpointInner>, InferenceEndpointInner> beginCreateOrUpdateAsync(
        String resourceGroupName, String workspaceName, String poolName, String endpointName,
        InferenceEndpointInner body) {
        Mono<Response<Flux<ByteBuffer>>> mono
            = createOrUpdateWithResponseAsync(resourceGroupName, workspaceName, poolName, endpointName, body);
        return this.client.<InferenceEndpointInner, InferenceEndpointInner>getLroResult(mono,
            this.client.getHttpPipeline(), InferenceEndpointInner.class, InferenceEndpointInner.class,
            this.client.getContext());
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private PollerFlux<PollResult<InferenceEndpointInner>, InferenceEndpointInner> beginCreateOrUpdateAsync(
        String resourceGroupName, String workspaceName, String poolName, String endpointName,
        InferenceEndpointInner body, Context context) {
        context = this.client.mergeContext(context);
        Mono<Response<Flux<ByteBuffer>>> mono
            = createOrUpdateWithResponseAsync(resourceGroupName, workspaceName, poolName, endpointName, body, context);
        return this.client.<InferenceEndpointInner, InferenceEndpointInner>getLroResult(mono,
            this.client.getHttpPipeline(), InferenceEndpointInner.class, InferenceEndpointInner.class, context);
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<InferenceEndpointInner>, InferenceEndpointInner> beginCreateOrUpdate(
        String resourceGroupName, String workspaceName, String poolName, String endpointName,
        InferenceEndpointInner body) {
        return this.beginCreateOrUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body)
            .getSyncPoller();
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<InferenceEndpointInner>, InferenceEndpointInner> beginCreateOrUpdate(
        String resourceGroupName, String workspaceName, String poolName, String endpointName,
        InferenceEndpointInner body, Context context) {
        return this.beginCreateOrUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body, context)
            .getSyncPoller();
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<InferenceEndpointInner> createOrUpdateAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, InferenceEndpointInner body) {
        return beginCreateOrUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body).last()
            .flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<InferenceEndpointInner> createOrUpdateAsync(String resourceGroupName, String workspaceName,
        String poolName, String endpointName, InferenceEndpointInner body, Context context) {
        return beginCreateOrUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body, context).last()
            .flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public InferenceEndpointInner createOrUpdate(String resourceGroupName, String workspaceName, String poolName,
        String endpointName, InferenceEndpointInner body) {
        return createOrUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body).block();
    }

    /**
     * Create or update InferenceEndpoint (asynchronous).
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Name of Azure Machine Learning workspace.
     * @param poolName InferencePool name.
     * @param endpointName InferenceEndpoint name.
     * @param body InferenceEndpoint entity to apply during operation.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public InferenceEndpointInner createOrUpdate(String resourceGroupName, String workspaceName, String poolName,
        String endpointName, InferenceEndpointInner body, Context context) {
        return createOrUpdateAsync(resourceGroupName, workspaceName, poolName, endpointName, body, context).block();
    }

    /**
     * Get the next page of items.
     * 
     * @param nextLink The URL to get the next list of items
     * 
     * The nextLink parameter.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities along with {@link PagedResponse} on successful completion
     * of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<PagedResponse<InferenceEndpointInner>> listNextSinglePageAsync(String nextLink) {
        if (nextLink == null) {
            return Mono.error(new IllegalArgumentException("Parameter nextLink is required and cannot be null."));
        }
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil.withContext(context -> service.listNext(nextLink, this.client.getEndpoint(), accept, context))
            .<PagedResponse<InferenceEndpointInner>>map(res -> new PagedResponseBase<>(res.getRequest(),
                res.getStatusCode(), res.getHeaders(), res.getValue().value(), res.getValue().nextLink(), null))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Get the next page of items.
     * 
     * @param nextLink The URL to get the next list of items
     * 
     * The nextLink parameter.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return a paginated list of InferenceEndpoint entities along with {@link PagedResponse} on successful completion
     * of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<PagedResponse<InferenceEndpointInner>> listNextSinglePageAsync(String nextLink, Context context) {
        if (nextLink == null) {
            return Mono.error(new IllegalArgumentException("Parameter nextLink is required and cannot be null."));
        }
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        final String accept = "application/json";
        context = this.client.mergeContext(context);
        return service.listNext(nextLink, this.client.getEndpoint(), accept, context)
            .map(res -> new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(),
                res.getValue().value(), res.getValue().nextLink(), null));
    }
}
