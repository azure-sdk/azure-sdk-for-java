// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.resourcemanager.machinelearning.implementation;

import com.azure.core.annotation.BodyParam;
import com.azure.core.annotation.Delete;
import com.azure.core.annotation.ExpectedResponses;
import com.azure.core.annotation.Get;
import com.azure.core.annotation.HeaderParam;
import com.azure.core.annotation.Headers;
import com.azure.core.annotation.Host;
import com.azure.core.annotation.HostParam;
import com.azure.core.annotation.PathParam;
import com.azure.core.annotation.Post;
import com.azure.core.annotation.Put;
import com.azure.core.annotation.QueryParam;
import com.azure.core.annotation.ReturnType;
import com.azure.core.annotation.ServiceInterface;
import com.azure.core.annotation.ServiceMethod;
import com.azure.core.annotation.UnexpectedResponseExceptionType;
import com.azure.core.http.rest.PagedFlux;
import com.azure.core.http.rest.PagedIterable;
import com.azure.core.http.rest.PagedResponse;
import com.azure.core.http.rest.PagedResponseBase;
import com.azure.core.http.rest.Response;
import com.azure.core.http.rest.RestProxy;
import com.azure.core.management.exception.ManagementException;
import com.azure.core.management.polling.PollResult;
import com.azure.core.util.BinaryData;
import com.azure.core.util.Context;
import com.azure.core.util.FluxUtil;
import com.azure.core.util.logging.ClientLogger;
import com.azure.core.util.polling.PollerFlux;
import com.azure.core.util.polling.SyncPoller;
import com.azure.resourcemanager.machinelearning.fluent.ConnectionsClient;
import com.azure.resourcemanager.machinelearning.fluent.models.EndpointDeploymentResourcePropertiesBasicResourceInner;
import com.azure.resourcemanager.machinelearning.fluent.models.EndpointModelPropertiesInner;
import com.azure.resourcemanager.machinelearning.fluent.models.EndpointModelsInner;
import com.azure.resourcemanager.machinelearning.models.EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult;
import java.nio.ByteBuffer;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;

/**
 * An instance of this class provides access to all the operations defined in ConnectionsClient.
 */
public final class ConnectionsClientImpl implements ConnectionsClient {
    /**
     * The proxy service used to perform REST calls.
     */
    private final ConnectionsService service;

    /**
     * The service client containing this operation class.
     */
    private final AzureMachineLearningWorkspacesImpl client;

    /**
     * Initializes an instance of ConnectionsClientImpl.
     * 
     * @param client the instance of the service client containing this operation class.
     */
    ConnectionsClientImpl(AzureMachineLearningWorkspacesImpl client) {
        this.service
            = RestProxy.create(ConnectionsService.class, client.getHttpPipeline(), client.getSerializerAdapter());
        this.client = client;
    }

    /**
     * The interface defining all the services for AzureMachineLearningWorkspacesConnections to be used by the proxy
     * service to perform REST calls.
     */
    @Host("{$host}")
    @ServiceInterface(name = "AzureMachineLearning")
    public interface ConnectionsService {
        @Headers({ "Content-Type: application/json" })
        @Get("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/deployments")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult>> listDeployments(
            @HostParam("$host") String endpoint, @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @QueryParam("api-version") String apiVersion,
            @QueryParam("proxy-api-version") String proxyApiVersion, @HeaderParam("Accept") String accept,
            Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/deployments")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Response<EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult> listDeploymentsSync(
            @HostParam("$host") String endpoint, @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @QueryParam("api-version") String apiVersion,
            @QueryParam("proxy-api-version") String proxyApiVersion, @HeaderParam("Accept") String accept,
            Context context);

        @Headers({ "Content-Type: application/json" })
        @Delete("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/deployments/{deploymentName}")
        @ExpectedResponses({ 202, 204 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<Flux<ByteBuffer>>> deleteDeployment(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @PathParam("deploymentName") String deploymentName,
            @QueryParam("api-version") String apiVersion, @QueryParam("proxy-api-version") String proxyApiVersion,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Delete("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/deployments/{deploymentName}")
        @ExpectedResponses({ 202, 204 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Response<BinaryData> deleteDeploymentSync(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @PathParam("deploymentName") String deploymentName,
            @QueryParam("api-version") String apiVersion, @QueryParam("proxy-api-version") String proxyApiVersion,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/deployments/{deploymentName}")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<EndpointDeploymentResourcePropertiesBasicResourceInner>> getDeployment(
            @HostParam("$host") String endpoint, @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @PathParam("deploymentName") String deploymentName,
            @QueryParam("api-version") String apiVersion, @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/deployments/{deploymentName}")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Response<EndpointDeploymentResourcePropertiesBasicResourceInner> getDeploymentSync(
            @HostParam("$host") String endpoint, @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @PathParam("deploymentName") String deploymentName,
            @QueryParam("api-version") String apiVersion, @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Put("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/deployments/{deploymentName}")
        @ExpectedResponses({ 200, 201 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<Flux<ByteBuffer>>> createOrUpdateDeployment(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @PathParam("deploymentName") String deploymentName,
            @QueryParam("api-version") String apiVersion, @QueryParam("proxy-api-version") String proxyApiVersion,
            @BodyParam("application/json") EndpointDeploymentResourcePropertiesBasicResourceInner body,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Put("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/deployments/{deploymentName}")
        @ExpectedResponses({ 200, 201 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Response<BinaryData> createOrUpdateDeploymentSync(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @PathParam("deploymentName") String deploymentName,
            @QueryParam("api-version") String apiVersion, @QueryParam("proxy-api-version") String proxyApiVersion,
            @BodyParam("application/json") EndpointDeploymentResourcePropertiesBasicResourceInner body,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/models")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<EndpointModelsInner>> getModels(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @QueryParam("api-version") String apiVersion,
            @QueryParam("proxy-api-version") String proxyApiVersion, @HeaderParam("Accept") String accept,
            Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/connections/{connectionName}/models")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Response<EndpointModelsInner> getModelsSync(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @PathParam("connectionName") String connectionName, @QueryParam("api-version") String apiVersion,
            @QueryParam("proxy-api-version") String proxyApiVersion, @HeaderParam("Accept") String accept,
            Context context);

        @Headers({ "Content-Type: application/json" })
        @Post("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/listConnectionModels")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<EndpointModelsInner>> getAllModels(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @QueryParam("api-version") String apiVersion, @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Post("/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/listConnectionModels")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Response<EndpointModelsInner> getAllModelsSync(@HostParam("$host") String endpoint,
            @PathParam("subscriptionId") String subscriptionId,
            @PathParam("resourceGroupName") String resourceGroupName, @PathParam("workspaceName") String workspaceName,
            @QueryParam("api-version") String apiVersion, @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("{nextLink}")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult>> listDeploymentsNext(
            @PathParam(value = "nextLink", encoded = true) String nextLink, @HostParam("$host") String endpoint,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("{nextLink}")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Response<EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult> listDeploymentsNextSync(
            @PathParam(value = "nextLink", encoded = true) String nextLink, @HostParam("$host") String endpoint,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("{nextLink}")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Mono<Response<EndpointModelsInner>> getModelsNext(
            @PathParam(value = "nextLink", encoded = true) String nextLink, @HostParam("$host") String endpoint,
            @HeaderParam("Accept") String accept, Context context);

        @Headers({ "Content-Type: application/json" })
        @Get("{nextLink}")
        @ExpectedResponses({ 200 })
        @UnexpectedResponseExceptionType(ManagementException.class)
        Response<EndpointModelsInner> getModelsNextSync(@PathParam(value = "nextLink", encoded = true) String nextLink,
            @HostParam("$host") String endpoint, @HeaderParam("Accept") String accept, Context context);
    }

    /**
     * Get all the deployments under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return all the deployments under the Azure OpenAI connection along with {@link PagedResponse} on successful
     * completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<PagedResponse<EndpointDeploymentResourcePropertiesBasicResourceInner>> listDeploymentsSinglePageAsync(
        String resourceGroupName, String workspaceName, String connectionName, String proxyApiVersion) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            return Mono.error(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.listDeployments(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, connectionName, this.client.getApiVersion(), proxyApiVersion, accept,
                context))
            .<PagedResponse<EndpointDeploymentResourcePropertiesBasicResourceInner>>map(
                res -> new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(),
                    res.getValue().value(), res.getValue().nextLink(), null))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Get all the deployments under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return all the deployments under the Azure OpenAI connection as paginated response with {@link PagedFlux}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    private PagedFlux<EndpointDeploymentResourcePropertiesBasicResourceInner> listDeploymentsAsync(
        String resourceGroupName, String workspaceName, String connectionName, String proxyApiVersion) {
        return new PagedFlux<>(
            () -> listDeploymentsSinglePageAsync(resourceGroupName, workspaceName, connectionName, proxyApiVersion),
            nextLink -> listDeploymentsNextSinglePageAsync(nextLink));
    }

    /**
     * Get all the deployments under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return all the deployments under the Azure OpenAI connection as paginated response with {@link PagedFlux}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    private PagedFlux<EndpointDeploymentResourcePropertiesBasicResourceInner>
        listDeploymentsAsync(String resourceGroupName, String workspaceName, String connectionName) {
        final String proxyApiVersion = null;
        return new PagedFlux<>(
            () -> listDeploymentsSinglePageAsync(resourceGroupName, workspaceName, connectionName, proxyApiVersion),
            nextLink -> listDeploymentsNextSinglePageAsync(nextLink));
    }

    /**
     * Get all the deployments under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return all the deployments under the Azure OpenAI connection along with {@link PagedResponse}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private PagedResponse<EndpointDeploymentResourcePropertiesBasicResourceInner> listDeploymentsSinglePage(
        String resourceGroupName, String workspaceName, String connectionName, String proxyApiVersion) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        final String accept = "application/json";
        Response<EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult> res
            = service.listDeploymentsSync(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
                workspaceName, connectionName, this.client.getApiVersion(), proxyApiVersion, accept, Context.NONE);
        return new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(), res.getValue().value(),
            res.getValue().nextLink(), null);
    }

    /**
     * Get all the deployments under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return all the deployments under the Azure OpenAI connection along with {@link PagedResponse}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private PagedResponse<EndpointDeploymentResourcePropertiesBasicResourceInner> listDeploymentsSinglePage(
        String resourceGroupName, String workspaceName, String connectionName, String proxyApiVersion,
        Context context) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        final String accept = "application/json";
        Response<EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult> res
            = service.listDeploymentsSync(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
                workspaceName, connectionName, this.client.getApiVersion(), proxyApiVersion, accept, context);
        return new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(), res.getValue().value(),
            res.getValue().nextLink(), null);
    }

    /**
     * Get all the deployments under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return all the deployments under the Azure OpenAI connection as paginated response with {@link PagedIterable}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    public PagedIterable<EndpointDeploymentResourcePropertiesBasicResourceInner>
        listDeployments(String resourceGroupName, String workspaceName, String connectionName) {
        final String proxyApiVersion = null;
        return new PagedIterable<>(
            () -> listDeploymentsSinglePage(resourceGroupName, workspaceName, connectionName, proxyApiVersion),
            nextLink -> listDeploymentsNextSinglePage(nextLink));
    }

    /**
     * Get all the deployments under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return all the deployments under the Azure OpenAI connection as paginated response with {@link PagedIterable}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    public PagedIterable<EndpointDeploymentResourcePropertiesBasicResourceInner> listDeployments(
        String resourceGroupName, String workspaceName, String connectionName, String proxyApiVersion,
        Context context) {
        return new PagedIterable<>(
            () -> listDeploymentsSinglePage(resourceGroupName, workspaceName, connectionName, proxyApiVersion, context),
            nextLink -> listDeploymentsNextSinglePage(nextLink, context));
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<Flux<ByteBuffer>>> deleteDeploymentWithResponseAsync(String resourceGroupName,
        String workspaceName, String connectionName, String deploymentName, String proxyApiVersion) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            return Mono.error(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        if (deploymentName == null) {
            return Mono.error(new IllegalArgumentException("Parameter deploymentName is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.deleteDeployment(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, connectionName, deploymentName, this.client.getApiVersion(),
                proxyApiVersion, accept, context))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Response<BinaryData> deleteDeploymentWithResponse(String resourceGroupName, String workspaceName,
        String connectionName, String deploymentName, String proxyApiVersion) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        if (deploymentName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter deploymentName is required and cannot be null."));
        }
        final String accept = "application/json";
        return service.deleteDeploymentSync(this.client.getEndpoint(), this.client.getSubscriptionId(),
            resourceGroupName, workspaceName, connectionName, deploymentName, this.client.getApiVersion(),
            proxyApiVersion, accept, Context.NONE);
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Response<BinaryData> deleteDeploymentWithResponse(String resourceGroupName, String workspaceName,
        String connectionName, String deploymentName, String proxyApiVersion, Context context) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        if (deploymentName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter deploymentName is required and cannot be null."));
        }
        final String accept = "application/json";
        return service.deleteDeploymentSync(this.client.getEndpoint(), this.client.getSubscriptionId(),
            resourceGroupName, workspaceName, connectionName, deploymentName, this.client.getApiVersion(),
            proxyApiVersion, accept, context);
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private PollerFlux<PollResult<Void>, Void> beginDeleteDeploymentAsync(String resourceGroupName,
        String workspaceName, String connectionName, String deploymentName, String proxyApiVersion) {
        Mono<Response<Flux<ByteBuffer>>> mono = deleteDeploymentWithResponseAsync(resourceGroupName, workspaceName,
            connectionName, deploymentName, proxyApiVersion);
        return this.client.<Void, Void>getLroResult(mono, this.client.getHttpPipeline(), Void.class, Void.class,
            this.client.getContext());
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private PollerFlux<PollResult<Void>, Void> beginDeleteDeploymentAsync(String resourceGroupName,
        String workspaceName, String connectionName, String deploymentName) {
        final String proxyApiVersion = null;
        Mono<Response<Flux<ByteBuffer>>> mono = deleteDeploymentWithResponseAsync(resourceGroupName, workspaceName,
            connectionName, deploymentName, proxyApiVersion);
        return this.client.<Void, Void>getLroResult(mono, this.client.getHttpPipeline(), Void.class, Void.class,
            this.client.getContext());
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<Void>, Void> beginDeleteDeployment(String resourceGroupName, String workspaceName,
        String connectionName, String deploymentName, String proxyApiVersion) {
        Response<BinaryData> response = deleteDeploymentWithResponse(resourceGroupName, workspaceName, connectionName,
            deploymentName, proxyApiVersion);
        return this.client.<Void, Void>getLroResult(response, Void.class, Void.class, Context.NONE);
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<Void>, Void> beginDeleteDeployment(String resourceGroupName, String workspaceName,
        String connectionName, String deploymentName) {
        final String proxyApiVersion = null;
        Response<BinaryData> response = deleteDeploymentWithResponse(resourceGroupName, workspaceName, connectionName,
            deploymentName, proxyApiVersion);
        return this.client.<Void, Void>getLroResult(response, Void.class, Void.class, Context.NONE);
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public SyncPoller<PollResult<Void>, Void> beginDeleteDeployment(String resourceGroupName, String workspaceName,
        String connectionName, String deploymentName, String proxyApiVersion, Context context) {
        Response<BinaryData> response = deleteDeploymentWithResponse(resourceGroupName, workspaceName, connectionName,
            deploymentName, proxyApiVersion, context);
        return this.client.<Void, Void>getLroResult(response, Void.class, Void.class, context);
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return A {@link Mono} that completes when a successful response is received.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Void> deleteDeploymentAsync(String resourceGroupName, String workspaceName, String connectionName,
        String deploymentName, String proxyApiVersion) {
        return beginDeleteDeploymentAsync(resourceGroupName, workspaceName, connectionName, deploymentName,
            proxyApiVersion).last().flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return A {@link Mono} that completes when a successful response is received.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Void> deleteDeploymentAsync(String resourceGroupName, String workspaceName, String connectionName,
        String deploymentName) {
        final String proxyApiVersion = null;
        return beginDeleteDeploymentAsync(resourceGroupName, workspaceName, connectionName, deploymentName,
            proxyApiVersion).last().flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public void deleteDeployment(String resourceGroupName, String workspaceName, String connectionName,
        String deploymentName) {
        final String proxyApiVersion = null;
        beginDeleteDeployment(resourceGroupName, workspaceName, connectionName, deploymentName, proxyApiVersion)
            .getFinalResult();
    }

    /**
     * Delete Azure OpenAI connection deployment resource by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public void deleteDeployment(String resourceGroupName, String workspaceName, String connectionName,
        String deploymentName, String proxyApiVersion, Context context) {
        beginDeleteDeployment(resourceGroupName, workspaceName, connectionName, deploymentName, proxyApiVersion,
            context).getFinalResult();
    }

    /**
     * Get deployments under the Azure OpenAI connection by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return deployments under the Azure OpenAI connection by name along with {@link Response} on successful
     * completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<EndpointDeploymentResourcePropertiesBasicResourceInner>> getDeploymentWithResponseAsync(
        String resourceGroupName, String workspaceName, String connectionName, String deploymentName) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            return Mono.error(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        if (deploymentName == null) {
            return Mono.error(new IllegalArgumentException("Parameter deploymentName is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.getDeployment(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, connectionName, deploymentName, this.client.getApiVersion(), accept,
                context))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Get deployments under the Azure OpenAI connection by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return deployments under the Azure OpenAI connection by name on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<EndpointDeploymentResourcePropertiesBasicResourceInner> getDeploymentAsync(String resourceGroupName,
        String workspaceName, String connectionName, String deploymentName) {
        return getDeploymentWithResponseAsync(resourceGroupName, workspaceName, connectionName, deploymentName)
            .flatMap(res -> Mono.justOrEmpty(res.getValue()));
    }

    /**
     * Get deployments under the Azure OpenAI connection by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return deployments under the Azure OpenAI connection by name along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<EndpointDeploymentResourcePropertiesBasicResourceInner> getDeploymentWithResponse(
        String resourceGroupName, String workspaceName, String connectionName, String deploymentName, Context context) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        if (deploymentName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter deploymentName is required and cannot be null."));
        }
        final String accept = "application/json";
        return service.getDeploymentSync(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
            workspaceName, connectionName, deploymentName, this.client.getApiVersion(), accept, context);
    }

    /**
     * Get deployments under the Azure OpenAI connection by name.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return deployments under the Azure OpenAI connection by name.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public EndpointDeploymentResourcePropertiesBasicResourceInner getDeployment(String resourceGroupName,
        String workspaceName, String connectionName, String deploymentName) {
        return getDeploymentWithResponse(resourceGroupName, workspaceName, connectionName, deploymentName, Context.NONE)
            .getValue();
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<Flux<ByteBuffer>>> createOrUpdateDeploymentWithResponseAsync(String resourceGroupName,
        String workspaceName, String connectionName, String deploymentName,
        EndpointDeploymentResourcePropertiesBasicResourceInner body, String proxyApiVersion) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            return Mono.error(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        if (deploymentName == null) {
            return Mono.error(new IllegalArgumentException("Parameter deploymentName is required and cannot be null."));
        }
        if (body == null) {
            return Mono.error(new IllegalArgumentException("Parameter body is required and cannot be null."));
        } else {
            body.validate();
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.createOrUpdateDeployment(this.client.getEndpoint(),
                this.client.getSubscriptionId(), resourceGroupName, workspaceName, connectionName, deploymentName,
                this.client.getApiVersion(), proxyApiVersion, body, accept, context))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Response<BinaryData> createOrUpdateDeploymentWithResponse(String resourceGroupName, String workspaceName,
        String connectionName, String deploymentName, EndpointDeploymentResourcePropertiesBasicResourceInner body,
        String proxyApiVersion) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        if (deploymentName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter deploymentName is required and cannot be null."));
        }
        if (body == null) {
            throw LOGGER.atError().log(new IllegalArgumentException("Parameter body is required and cannot be null."));
        } else {
            body.validate();
        }
        final String accept = "application/json";
        return service.createOrUpdateDeploymentSync(this.client.getEndpoint(), this.client.getSubscriptionId(),
            resourceGroupName, workspaceName, connectionName, deploymentName, this.client.getApiVersion(),
            proxyApiVersion, body, accept, Context.NONE);
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Response<BinaryData> createOrUpdateDeploymentWithResponse(String resourceGroupName, String workspaceName,
        String connectionName, String deploymentName, EndpointDeploymentResourcePropertiesBasicResourceInner body,
        String proxyApiVersion, Context context) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        if (deploymentName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter deploymentName is required and cannot be null."));
        }
        if (body == null) {
            throw LOGGER.atError().log(new IllegalArgumentException("Parameter body is required and cannot be null."));
        } else {
            body.validate();
        }
        final String accept = "application/json";
        return service.createOrUpdateDeploymentSync(this.client.getEndpoint(), this.client.getSubscriptionId(),
            resourceGroupName, workspaceName, connectionName, deploymentName, this.client.getApiVersion(),
            proxyApiVersion, body, accept, context);
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private
        PollerFlux<PollResult<EndpointDeploymentResourcePropertiesBasicResourceInner>, EndpointDeploymentResourcePropertiesBasicResourceInner>
        beginCreateOrUpdateDeploymentAsync(String resourceGroupName, String workspaceName, String connectionName,
            String deploymentName, EndpointDeploymentResourcePropertiesBasicResourceInner body,
            String proxyApiVersion) {
        Mono<Response<Flux<ByteBuffer>>> mono = createOrUpdateDeploymentWithResponseAsync(resourceGroupName,
            workspaceName, connectionName, deploymentName, body, proxyApiVersion);
        return this.client
            .<EndpointDeploymentResourcePropertiesBasicResourceInner, EndpointDeploymentResourcePropertiesBasicResourceInner>getLroResult(
                mono, this.client.getHttpPipeline(), EndpointDeploymentResourcePropertiesBasicResourceInner.class,
                EndpointDeploymentResourcePropertiesBasicResourceInner.class, this.client.getContext());
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link PollerFlux} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    private
        PollerFlux<PollResult<EndpointDeploymentResourcePropertiesBasicResourceInner>, EndpointDeploymentResourcePropertiesBasicResourceInner>
        beginCreateOrUpdateDeploymentAsync(String resourceGroupName, String workspaceName, String connectionName,
            String deploymentName, EndpointDeploymentResourcePropertiesBasicResourceInner body) {
        final String proxyApiVersion = null;
        Mono<Response<Flux<ByteBuffer>>> mono = createOrUpdateDeploymentWithResponseAsync(resourceGroupName,
            workspaceName, connectionName, deploymentName, body, proxyApiVersion);
        return this.client
            .<EndpointDeploymentResourcePropertiesBasicResourceInner, EndpointDeploymentResourcePropertiesBasicResourceInner>getLroResult(
                mono, this.client.getHttpPipeline(), EndpointDeploymentResourcePropertiesBasicResourceInner.class,
                EndpointDeploymentResourcePropertiesBasicResourceInner.class, this.client.getContext());
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public
        SyncPoller<PollResult<EndpointDeploymentResourcePropertiesBasicResourceInner>, EndpointDeploymentResourcePropertiesBasicResourceInner>
        beginCreateOrUpdateDeployment(String resourceGroupName, String workspaceName, String connectionName,
            String deploymentName, EndpointDeploymentResourcePropertiesBasicResourceInner body,
            String proxyApiVersion) {
        Response<BinaryData> response = createOrUpdateDeploymentWithResponse(resourceGroupName, workspaceName,
            connectionName, deploymentName, body, proxyApiVersion);
        return this.client
            .<EndpointDeploymentResourcePropertiesBasicResourceInner, EndpointDeploymentResourcePropertiesBasicResourceInner>getLroResult(
                response, EndpointDeploymentResourcePropertiesBasicResourceInner.class,
                EndpointDeploymentResourcePropertiesBasicResourceInner.class, Context.NONE);
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public
        SyncPoller<PollResult<EndpointDeploymentResourcePropertiesBasicResourceInner>, EndpointDeploymentResourcePropertiesBasicResourceInner>
        beginCreateOrUpdateDeployment(String resourceGroupName, String workspaceName, String connectionName,
            String deploymentName, EndpointDeploymentResourcePropertiesBasicResourceInner body) {
        final String proxyApiVersion = null;
        Response<BinaryData> response = createOrUpdateDeploymentWithResponse(resourceGroupName, workspaceName,
            connectionName, deploymentName, body, proxyApiVersion);
        return this.client
            .<EndpointDeploymentResourcePropertiesBasicResourceInner, EndpointDeploymentResourcePropertiesBasicResourceInner>getLroResult(
                response, EndpointDeploymentResourcePropertiesBasicResourceInner.class,
                EndpointDeploymentResourcePropertiesBasicResourceInner.class, Context.NONE);
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the {@link SyncPoller} for polling of long-running operation.
     */
    @ServiceMethod(returns = ReturnType.LONG_RUNNING_OPERATION)
    public
        SyncPoller<PollResult<EndpointDeploymentResourcePropertiesBasicResourceInner>, EndpointDeploymentResourcePropertiesBasicResourceInner>
        beginCreateOrUpdateDeployment(String resourceGroupName, String workspaceName, String connectionName,
            String deploymentName, EndpointDeploymentResourcePropertiesBasicResourceInner body, String proxyApiVersion,
            Context context) {
        Response<BinaryData> response = createOrUpdateDeploymentWithResponse(resourceGroupName, workspaceName,
            connectionName, deploymentName, body, proxyApiVersion, context);
        return this.client
            .<EndpointDeploymentResourcePropertiesBasicResourceInner, EndpointDeploymentResourcePropertiesBasicResourceInner>getLroResult(
                response, EndpointDeploymentResourcePropertiesBasicResourceInner.class,
                EndpointDeploymentResourcePropertiesBasicResourceInner.class, context);
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<EndpointDeploymentResourcePropertiesBasicResourceInner> createOrUpdateDeploymentAsync(
        String resourceGroupName, String workspaceName, String connectionName, String deploymentName,
        EndpointDeploymentResourcePropertiesBasicResourceInner body, String proxyApiVersion) {
        return beginCreateOrUpdateDeploymentAsync(resourceGroupName, workspaceName, connectionName, deploymentName,
            body, proxyApiVersion).last().flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<EndpointDeploymentResourcePropertiesBasicResourceInner> createOrUpdateDeploymentAsync(
        String resourceGroupName, String workspaceName, String connectionName, String deploymentName,
        EndpointDeploymentResourcePropertiesBasicResourceInner body) {
        final String proxyApiVersion = null;
        return beginCreateOrUpdateDeploymentAsync(resourceGroupName, workspaceName, connectionName, deploymentName,
            body, proxyApiVersion).last().flatMap(this.client::getLroFinalResultOrError);
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public EndpointDeploymentResourcePropertiesBasicResourceInner createOrUpdateDeployment(String resourceGroupName,
        String workspaceName, String connectionName, String deploymentName,
        EndpointDeploymentResourcePropertiesBasicResourceInner body) {
        final String proxyApiVersion = null;
        return beginCreateOrUpdateDeployment(resourceGroupName, workspaceName, connectionName, deploymentName, body,
            proxyApiVersion).getFinalResult();
    }

    /**
     * Create or update Azure OpenAI connection deployment resource with the specified parameters.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param deploymentName Name of the deployment resource.
     * @param body deployment object.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public EndpointDeploymentResourcePropertiesBasicResourceInner createOrUpdateDeployment(String resourceGroupName,
        String workspaceName, String connectionName, String deploymentName,
        EndpointDeploymentResourcePropertiesBasicResourceInner body, String proxyApiVersion, Context context) {
        return beginCreateOrUpdateDeployment(resourceGroupName, workspaceName, connectionName, deploymentName, body,
            proxyApiVersion, context).getFinalResult();
    }

    /**
     * Get available models under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return available models under the Azure OpenAI connection along with {@link PagedResponse} on successful
     * completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<PagedResponse<EndpointModelPropertiesInner>> getModelsSinglePageAsync(String resourceGroupName,
        String workspaceName, String connectionName, String proxyApiVersion) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            return Mono.error(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.getModels(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, connectionName, this.client.getApiVersion(), proxyApiVersion, accept,
                context))
            .<PagedResponse<EndpointModelPropertiesInner>>map(res -> new PagedResponseBase<>(res.getRequest(),
                res.getStatusCode(), res.getHeaders(), res.getValue().value(), res.getValue().nextLink(), null))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Get available models under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return available models under the Azure OpenAI connection as paginated response with {@link PagedFlux}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    private PagedFlux<EndpointModelPropertiesInner> getModelsAsync(String resourceGroupName, String workspaceName,
        String connectionName, String proxyApiVersion) {
        return new PagedFlux<>(
            () -> getModelsSinglePageAsync(resourceGroupName, workspaceName, connectionName, proxyApiVersion),
            nextLink -> getModelsNextSinglePageAsync(nextLink));
    }

    /**
     * Get available models under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return available models under the Azure OpenAI connection as paginated response with {@link PagedFlux}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    private PagedFlux<EndpointModelPropertiesInner> getModelsAsync(String resourceGroupName, String workspaceName,
        String connectionName) {
        final String proxyApiVersion = null;
        return new PagedFlux<>(
            () -> getModelsSinglePageAsync(resourceGroupName, workspaceName, connectionName, proxyApiVersion),
            nextLink -> getModelsNextSinglePageAsync(nextLink));
    }

    /**
     * Get available models under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return available models under the Azure OpenAI connection along with {@link PagedResponse}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private PagedResponse<EndpointModelPropertiesInner> getModelsSinglePage(String resourceGroupName,
        String workspaceName, String connectionName, String proxyApiVersion) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        final String accept = "application/json";
        Response<EndpointModelsInner> res
            = service.getModelsSync(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
                workspaceName, connectionName, this.client.getApiVersion(), proxyApiVersion, accept, Context.NONE);
        return new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(), res.getValue().value(),
            res.getValue().nextLink(), null);
    }

    /**
     * Get available models under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return available models under the Azure OpenAI connection along with {@link PagedResponse}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private PagedResponse<EndpointModelPropertiesInner> getModelsSinglePage(String resourceGroupName,
        String workspaceName, String connectionName, String proxyApiVersion, Context context) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        if (connectionName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter connectionName is required and cannot be null."));
        }
        final String accept = "application/json";
        Response<EndpointModelsInner> res
            = service.getModelsSync(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
                workspaceName, connectionName, this.client.getApiVersion(), proxyApiVersion, accept, context);
        return new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(), res.getValue().value(),
            res.getValue().nextLink(), null);
    }

    /**
     * Get available models under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return available models under the Azure OpenAI connection as paginated response with {@link PagedIterable}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    public PagedIterable<EndpointModelPropertiesInner> getModels(String resourceGroupName, String workspaceName,
        String connectionName) {
        final String proxyApiVersion = null;
        return new PagedIterable<>(
            () -> getModelsSinglePage(resourceGroupName, workspaceName, connectionName, proxyApiVersion),
            nextLink -> getModelsNextSinglePage(nextLink));
    }

    /**
     * Get available models under the Azure OpenAI connection.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param connectionName Friendly name of the workspace connection.
     * @param proxyApiVersion Api version used by proxy call.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return available models under the Azure OpenAI connection as paginated response with {@link PagedIterable}.
     */
    @ServiceMethod(returns = ReturnType.COLLECTION)
    public PagedIterable<EndpointModelPropertiesInner> getModels(String resourceGroupName, String workspaceName,
        String connectionName, String proxyApiVersion, Context context) {
        return new PagedIterable<>(
            () -> getModelsSinglePage(resourceGroupName, workspaceName, connectionName, proxyApiVersion, context),
            nextLink -> getModelsNextSinglePage(nextLink, context));
    }

    /**
     * Get models under the Azure ML workspace for all Azure OpenAI connections that the user can deploy.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return models under the Azure ML workspace for all Azure OpenAI connections that the user can deploy along with
     * {@link Response} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<Response<EndpointModelsInner>> getAllModelsWithResponseAsync(String resourceGroupName,
        String workspaceName) {
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            return Mono.error(new IllegalArgumentException(
                "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            return Mono
                .error(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            return Mono.error(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.getAllModels(this.client.getEndpoint(), this.client.getSubscriptionId(),
                resourceGroupName, workspaceName, this.client.getApiVersion(), accept, context))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Get models under the Azure ML workspace for all Azure OpenAI connections that the user can deploy.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return models under the Azure ML workspace for all Azure OpenAI connections that the user can deploy on
     * successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<EndpointModelsInner> getAllModelsAsync(String resourceGroupName, String workspaceName) {
        return getAllModelsWithResponseAsync(resourceGroupName, workspaceName)
            .flatMap(res -> Mono.justOrEmpty(res.getValue()));
    }

    /**
     * Get models under the Azure ML workspace for all Azure OpenAI connections that the user can deploy.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return models under the Azure ML workspace for all Azure OpenAI connections that the user can deploy along with
     * {@link Response}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public Response<EndpointModelsInner> getAllModelsWithResponse(String resourceGroupName, String workspaceName,
        Context context) {
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        if (this.client.getSubscriptionId() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getSubscriptionId() is required and cannot be null."));
        }
        if (resourceGroupName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter resourceGroupName is required and cannot be null."));
        }
        if (workspaceName == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter workspaceName is required and cannot be null."));
        }
        final String accept = "application/json";
        return service.getAllModelsSync(this.client.getEndpoint(), this.client.getSubscriptionId(), resourceGroupName,
            workspaceName, this.client.getApiVersion(), accept, context);
    }

    /**
     * Get models under the Azure ML workspace for all Azure OpenAI connections that the user can deploy.
     * 
     * @param resourceGroupName The name of the resource group. The name is case insensitive.
     * @param workspaceName Azure Machine Learning Workspace Name.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return models under the Azure ML workspace for all Azure OpenAI connections that the user can deploy.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    public EndpointModelsInner getAllModels(String resourceGroupName, String workspaceName) {
        return getAllModelsWithResponse(resourceGroupName, workspaceName, Context.NONE).getValue();
    }

    /**
     * Get the next page of items.
     * 
     * @param nextLink The URL to get the next list of items.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link PagedResponse} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<PagedResponse<EndpointDeploymentResourcePropertiesBasicResourceInner>>
        listDeploymentsNextSinglePageAsync(String nextLink) {
        if (nextLink == null) {
            return Mono.error(new IllegalArgumentException("Parameter nextLink is required and cannot be null."));
        }
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.listDeploymentsNext(nextLink, this.client.getEndpoint(), accept, context))
            .<PagedResponse<EndpointDeploymentResourcePropertiesBasicResourceInner>>map(
                res -> new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(),
                    res.getValue().value(), res.getValue().nextLink(), null))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Get the next page of items.
     * 
     * @param nextLink The URL to get the next list of items.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link PagedResponse}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private PagedResponse<EndpointDeploymentResourcePropertiesBasicResourceInner>
        listDeploymentsNextSinglePage(String nextLink) {
        if (nextLink == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter nextLink is required and cannot be null."));
        }
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        final String accept = "application/json";
        Response<EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult> res
            = service.listDeploymentsNextSync(nextLink, this.client.getEndpoint(), accept, Context.NONE);
        return new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(), res.getValue().value(),
            res.getValue().nextLink(), null);
    }

    /**
     * Get the next page of items.
     * 
     * @param nextLink The URL to get the next list of items.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link PagedResponse}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private PagedResponse<EndpointDeploymentResourcePropertiesBasicResourceInner>
        listDeploymentsNextSinglePage(String nextLink, Context context) {
        if (nextLink == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter nextLink is required and cannot be null."));
        }
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        final String accept = "application/json";
        Response<EndpointDeploymentResourcePropertiesBasicResourceArmPaginatedResult> res
            = service.listDeploymentsNextSync(nextLink, this.client.getEndpoint(), accept, context);
        return new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(), res.getValue().value(),
            res.getValue().nextLink(), null);
    }

    /**
     * Get the next page of items.
     * 
     * @param nextLink The URL to get the next list of items.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link PagedResponse} on successful completion of {@link Mono}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private Mono<PagedResponse<EndpointModelPropertiesInner>> getModelsNextSinglePageAsync(String nextLink) {
        if (nextLink == null) {
            return Mono.error(new IllegalArgumentException("Parameter nextLink is required and cannot be null."));
        }
        if (this.client.getEndpoint() == null) {
            return Mono.error(
                new IllegalArgumentException("Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        final String accept = "application/json";
        return FluxUtil
            .withContext(context -> service.getModelsNext(nextLink, this.client.getEndpoint(), accept, context))
            .<PagedResponse<EndpointModelPropertiesInner>>map(res -> new PagedResponseBase<>(res.getRequest(),
                res.getStatusCode(), res.getHeaders(), res.getValue().value(), res.getValue().nextLink(), null))
            .contextWrite(context -> context.putAll(FluxUtil.toReactorContext(this.client.getContext()).readOnly()));
    }

    /**
     * Get the next page of items.
     * 
     * @param nextLink The URL to get the next list of items.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link PagedResponse}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private PagedResponse<EndpointModelPropertiesInner> getModelsNextSinglePage(String nextLink) {
        if (nextLink == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter nextLink is required and cannot be null."));
        }
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        final String accept = "application/json";
        Response<EndpointModelsInner> res
            = service.getModelsNextSync(nextLink, this.client.getEndpoint(), accept, Context.NONE);
        return new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(), res.getValue().value(),
            res.getValue().nextLink(), null);
    }

    /**
     * Get the next page of items.
     * 
     * @param nextLink The URL to get the next list of items.
     * @param context The context to associate with this operation.
     * @throws IllegalArgumentException thrown if parameters fail the validation.
     * @throws ManagementException thrown if the request is rejected by server.
     * @throws RuntimeException all other wrapped checked exceptions if the request fails to be sent.
     * @return the response body along with {@link PagedResponse}.
     */
    @ServiceMethod(returns = ReturnType.SINGLE)
    private PagedResponse<EndpointModelPropertiesInner> getModelsNextSinglePage(String nextLink, Context context) {
        if (nextLink == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException("Parameter nextLink is required and cannot be null."));
        }
        if (this.client.getEndpoint() == null) {
            throw LOGGER.atError()
                .log(new IllegalArgumentException(
                    "Parameter this.client.getEndpoint() is required and cannot be null."));
        }
        final String accept = "application/json";
        Response<EndpointModelsInner> res
            = service.getModelsNextSync(nextLink, this.client.getEndpoint(), accept, context);
        return new PagedResponseBase<>(res.getRequest(), res.getStatusCode(), res.getHeaders(), res.getValue().value(),
            res.getValue().nextLink(), null);
    }

    private static final ClientLogger LOGGER = new ClientLogger(ConnectionsClientImpl.class);
}
