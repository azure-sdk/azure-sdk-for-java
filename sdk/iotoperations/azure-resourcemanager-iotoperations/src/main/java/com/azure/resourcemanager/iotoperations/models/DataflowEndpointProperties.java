// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) TypeSpec Code Generator.

package com.azure.resourcemanager.iotoperations.models;

import com.azure.core.annotation.Immutable;
import com.azure.json.JsonReader;
import com.azure.json.JsonSerializable;
import com.azure.json.JsonToken;
import com.azure.json.JsonWriter;
import java.io.IOException;

/**
 * DataflowEndpoint Resource properties. NOTE - Only one type of endpoint is supported for one Resource.
 */
@Immutable
public class DataflowEndpointProperties implements JsonSerializable<DataflowEndpointProperties> {
    /*
     * Endpoint Type.
     */
    private EndpointType endpointType = EndpointType.fromString("DataflowEndpointProperties");

    /*
     * The status of the last operation.
     */
    private ProvisioningState provisioningState;

    /**
     * Creates an instance of DataflowEndpointProperties class.
     */
    public DataflowEndpointProperties() {
    }

    /**
     * Get the endpointType property: Endpoint Type.
     * 
     * @return the endpointType value.
     */
    public EndpointType endpointType() {
        return this.endpointType;
    }

    /**
     * Get the provisioningState property: The status of the last operation.
     * 
     * @return the provisioningState value.
     */
    public ProvisioningState provisioningState() {
        return this.provisioningState;
    }

    /**
     * Set the provisioningState property: The status of the last operation.
     * 
     * @param provisioningState the provisioningState value to set.
     * @return the DataflowEndpointProperties object itself.
     */
    DataflowEndpointProperties withProvisioningState(ProvisioningState provisioningState) {
        this.provisioningState = provisioningState;
        return this;
    }

    /**
     * Validates the instance.
     * 
     * @throws IllegalArgumentException thrown if the instance is not valid.
     */
    public void validate() {
    }

    /**
     * {@inheritDoc}
     */
    @Override
    public JsonWriter toJson(JsonWriter jsonWriter) throws IOException {
        jsonWriter.writeStartObject();
        jsonWriter.writeStringField("endpointType", this.endpointType == null ? null : this.endpointType.toString());
        return jsonWriter.writeEndObject();
    }

    /**
     * Reads an instance of DataflowEndpointProperties from the JsonReader.
     * 
     * @param jsonReader The JsonReader being read.
     * @return An instance of DataflowEndpointProperties if the JsonReader was pointing to an instance of it, or null if
     * it was pointing to JSON null.
     * @throws IOException If an error occurs while reading the DataflowEndpointProperties.
     */
    public static DataflowEndpointProperties fromJson(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            String discriminatorValue = null;
            try (JsonReader readerToUse = reader.bufferObject()) {
                readerToUse.nextToken(); // Prepare for reading
                while (readerToUse.nextToken() != JsonToken.END_OBJECT) {
                    String fieldName = readerToUse.getFieldName();
                    readerToUse.nextToken();
                    if ("endpointType".equals(fieldName)) {
                        discriminatorValue = readerToUse.getString();
                        break;
                    } else {
                        readerToUse.skipChildren();
                    }
                }
                // Use the discriminator value to determine which subtype should be deserialized.
                if ("DataExplorer".equals(discriminatorValue)) {
                    return DataExplorerEndpoint.fromJson(readerToUse.reset());
                } else if ("DataLakeStorage".equals(discriminatorValue)) {
                    return DataLakeStorageEndpoint.fromJson(readerToUse.reset());
                } else if ("FabricOneLake".equals(discriminatorValue)) {
                    return FabricOneLakeEndpoint.fromJson(readerToUse.reset());
                } else if ("Kafka".equals(discriminatorValue)) {
                    return KafkaEndpoint.fromJson(readerToUse.reset());
                } else if ("LocalStorage".equals(discriminatorValue)) {
                    return LocalStorageEndpoint.fromJson(readerToUse.reset());
                } else if ("Mqtt".equals(discriminatorValue)) {
                    return MqttEndpoint.fromJson(readerToUse.reset());
                } else {
                    return fromJsonKnownDiscriminator(readerToUse.reset());
                }
            }
        });
    }

    static DataflowEndpointProperties fromJsonKnownDiscriminator(JsonReader jsonReader) throws IOException {
        return jsonReader.readObject(reader -> {
            DataflowEndpointProperties deserializedDataflowEndpointProperties = new DataflowEndpointProperties();
            while (reader.nextToken() != JsonToken.END_OBJECT) {
                String fieldName = reader.getFieldName();
                reader.nextToken();

                if ("endpointType".equals(fieldName)) {
                    deserializedDataflowEndpointProperties.endpointType = EndpointType.fromString(reader.getString());
                } else if ("provisioningState".equals(fieldName)) {
                    deserializedDataflowEndpointProperties.provisioningState
                        = ProvisioningState.fromString(reader.getString());
                } else {
                    reader.skipChildren();
                }
            }

            return deserializedDataflowEndpointProperties;
        });
    }
}
